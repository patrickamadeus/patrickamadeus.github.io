---
layout: homepage
---

## About Me

- I am a **PhD student** at [MBZUAI](https://mbzuai.ac.ae/) ğŸ¤–, where I focus on multimodal NLP topics, particularly vision-language interaction. I also work on vision-language alignment, unified (any) multimodal models, and large-scale evaluations. I am advised by [Alham Fikri Aji](https://scholar.google.com/citations?user=0Cyfqv4AAAAJ&hl=en) and [Yova Kementchedjhieva](https://scholar.google.com/citations?user=Br-FqfIAAAAJ&hl=en).

- Previously, I was a **Research Engineer** at SMU ğŸ‡¸ğŸ‡¬ working at the intersection of multilingual and multimodal interpretation under [ Chong-Wah Ngo](https://scholar.google.com/citations?user=HM39HrUAAAAJ&hl=en). Before that, I earned my bachelorâ€™s degree in CS from Institut Teknologi Bandung, where I worked under [Ayu Purwarianti](https://scholar.google.com/citations?user=8jUro_cAAAAJ&hl=en) on explainable multimodal synthetic data generation. 

- In addition to research, I also do AI engineering for various use cases. You can see my other experiences [here](https://www.linkedin.com/in/patrickamadeus/).

- Further, I plan to specialize my studies in developing methods to better align different modalities, with the goal of mitigating modality imbalance, especially in the avenue of unified multimodal models.

## Research Interests

During my studies and prior experience, I have often worked on topics including, but not limited to, the following:
1.	**Modality Utilization:**
I study how multimodal models allocate attention and capacity across modalities, and why they often under-utilize informative signals in favor of dominant ones. My work focuses on understanding when and why models fail to fully exploit available modalities, leading to sparse utilization, shortcut learning, hallucination, or language over-reliance. I aim to discover methods that encourage adaptive, task-aware modality usage in unified multimodal architectures.
2.	**LLM/VLM Alignment:**
To mitigate the above-mentioned problems, I work on both training and non-training adaptations to improve cross-modal grounding and utilization. My current work includes training and scaling novel VLM architectures to enhance modality-specific abilities, applying test-time scaling for better fine-grained visual understanding, and curating multimodal reward models.
3.	**Action-Conditioned Multimodal Systems:**
I am also interested in multimodal systems that operate under action-conditioned settings, where perception, language, and decision-making are tightly coupled. Currently, I am exploring how action feedback and interaction signals (e.g., world models, subspace alignment approaches such as C3 and ReAlign) can guide more grounded and consistent modality utilization. The broader goal is to move from static understanding toward systems that reason, plan, and adapt through active interaction.
4.	**Large-Scale Evaluations:**
Last but not least, I also often investigate model robustness under varying resource conditions and modality availability. This involves designing evaluation protocols that probe modality reliance, cross-modal consistency, and general inclusivity (multilingual/multicultural).

## Updates

- **[Feb. 2026]** 2 Papers accepted to CVPR 2026! (1 main, 1 findings)
- **[Nov. 2025]** Our study exposing the confusion of VLMs in cultural-conflict visual scenario is up on [arXiv](https://arxiv.org/abs/2511.17004)!
- **[Dec. 2025]** [M4-RAG](https://www.arxiv.org/abs/2512.05959) is out on arXiv! We present an evaluation of how multimodal knowledge enrichment helps model in tackling multilingual query. Spoiler, it does not always help... ğŸ¤¯
- **[Oct. 2025]** [Entropy2Vec](https://arxiv.org/abs/2509.05060) got accepted into MRL Workshop @ EMNLP 2025 ğŸŒğŸ‡¨ğŸ‡³!
- **[July 2025]** [Seeing Culture Benchmark](https://seeingculture-benchmark.github.io) is accepted to EMNLP 2025 ğŸ‡¨ğŸ‡³! On to the next one with SMU Multimedia team ğŸ’ª
- **[May. 2025]** [DataRubrics](https://arxiv.org/abs/2506.01789) is now on arXiv! We propose a unified scorecard to evaluate data quality on multi-faceted metrics.
- **[Apr. 2025]** [WorldCuisines](https://worldcuisines.github.io/) receives Best Theme Paper award at NAACL 2025! ğŸ‰ğŸŒğŸ½ï¸  
- **[Mar. 2025]** Admitted to the Fall 2025 cohort of the **MBZUAI PhD program in NLP**! ğŸ“š
- **[Jan. 2025]** [WorldCuisines](https://worldcuisines.github.io/) and *ProxyLM* are accepted to NAACL 2025 ğŸ‡ºğŸ‡¸ ğŸ–ï¸  
- **[Nov. 2024]** My first first-author [paper](https://arxiv.org/abs/2409.14785), a VL synthetic data generation framework, is accepted to COLING 2025 ğŸ‰  
- **[Oct. 2024]** [WorldCuisines](https://worldcuisines.github.io/), the largest multicultural VL food benchmark, is released. Honored to co-lead the project ğŸ¥˜  
- **[Sep. 2024]** [SEACrowd](https://arxiv.org/abs/2406.10118) is accepted to EMNLP 2024! ğŸ‡ºğŸ‡¸

{% include_relative _includes/publications.md %}

{% include_relative _includes/services.md %}
